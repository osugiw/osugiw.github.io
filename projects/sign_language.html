<!DOCTYPE HTML>
<html>
	<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Sugiarto Wibowo</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="" />
	<meta name="keywords" content="" />
	<meta name="author" content="" />

  <!-- Facebook and Twitter integration -->
	<meta property="og:title" content=""/>
	<meta property="og:image" content=""/>
	<meta property="og:url" content=""/>
	<meta property="og:site_name" content=""/>
	<meta property="og:description" content=""/>
	<meta name="twitter:title" content="" />
	<meta name="twitter:image" content="" />
	<meta name="twitter:url" content="" />
	<meta name="twitter:card" content="" />

	<!-- Place favicon.ico and apple-touch-icon.png in the root directory -->
	<link rel="shortcut icon" href="../images/favicon.ico">

	<link href="https://fonts.googleapis.com/css?family=Quicksand:300,400,500,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,700" rel="stylesheet">
	
	<!-- Animate.css -->
	<link rel="stylesheet" href="../css/animate.css">
	<!-- Icomoon Icon Fonts-->
	<link rel="stylesheet" href="../css/icomoon.css">
	<!-- Bootstrap  -->
	<link rel="stylesheet" href="../css/bootstrap.css">
	<!-- Flexslider  -->
	<link rel="stylesheet" href="../css/flexslider.css">
	<!-- Flaticons  -->
	<link rel="stylesheet" href="../fonts/flaticon/font/flaticon.css">
	<!-- Owl Carousel -->
	<link rel="stylesheet" href="../css/owl.carousel.min.css">
	<link rel="stylesheet" href="../css/owl.theme.default.min.css">
	<!-- Theme style  -->
	<link rel="stylesheet" href="../css/style.css">

	<!-- Modernizr JS -->
	<script src="../js/modernizr-2.6.2.min.js"></script>
	<!-- FOR IE9 below -->
	<!--[if lt IE 9]>
	<script src="js/respond.min.js"></script>
	<![endif]-->

	</head>
	<body>
	<div id="colorlib-page">
		<div class="container-wrap">
		<a href="../index.html" class="js-colorlib-nav-toggle colorlib-nav-toggle" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><i></i></a>
		<aside id="colorlib-aside" role="complementary" class="border js-fullheight">
			<div class="text-center">
				<div class="author-img" style="background-image: url('https://media.licdn.com/dms/image/v2/D5603AQE_PoxdPqhXRw/profile-displayphoto-scale_400_400/B56ZkGf6deHcAo-/0/1756750670009?e=1772064000&v=beta&t=y12NcD47DUDci3Nd9aKXpck3Onu3fZc0_OhjBK9bUWY');"></div>
				<h1 id="colorlib-logo"><a href="index.html">Sugiarto Wibowo</a></h1>
				<span class="position">
					<a href="#">ECE Graduate Student</a>
					<p>Rice University, Houston, TX</p>
				</span>
			</div>
			<nav id="colorlib-main-menu" role="navigation" class="navbar">
				<ul>
					<li><a class="text-center" href="../index.html">Back to Main Page</a></li>
				</ul>
			</nav>

			<div class="colorlib-footer">
				<p><small>&copy; <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
Copyright <script>document.write(new Date().getFullYear());</script> All rights reserved. Made with <i class="icon-heart" aria-hidden="true"></i> by <a href="https://colorlib.com" target="_blank">Colorlib</a>
<!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. --> </span><span>Background Images: <a href="https://unsplash.com/" target="_blank">Unsplash.com</a></span></small></p>
				<ul>
					<li><a href="https://github.com/osugiw"><i class="icon-social-github"></i></a></li>
					<li><a href="https://www.instagram.com/osugi_w/"><i class="icon-instagram"></i></a></li>
					<li><a href="https://www.linkedin.com/in/sugiarto-wibowo-osw/"><i class="icon-linkedin2"></i></a></li>
					<li><a href="https://scholar.google.com/citations?user=nWA4pUsAAAAJ&hl=en"><svg class="icon icon-googlescholar"><use xlink:href="#icon-googlescholar"></use></svg></i></a></li>
				</ul>
			</div>
			<div class="colorlib-footer">
				<script src="https://static.elfsight.com/platform/platform.js" data-use-service-core defer></script>
				<div class="elfsight-app-7a2c62c7-7263-4410-8572-35487f42acfe" data-elfsight-app-lazy></div>
			</div>

		</aside>

		<div id="colorlib-main">
			<section class="colorlib-about" data-section="sign_language-content">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-12">
							<div class="row row-bottom-padded-sm animate-box" data-animate-effect="fadeInLeft">
								<div class="col-md-12">
									<div class="about-desc">
										<span class="heading-meta">Artificial Intelligence | Embedded Programming</span>
										<h1 class="colorlib-heading" style="border-bottom: 3px solid #eeeeee;">Sign Language Detection using YOLOv5</h2>
									</div>
								</div>
							</div>
						</div>
					</div>

					<!-- Abstract  -->
					<div class="row animate-box" style="margin-bottom: 2.5em;" data-animate-effect="fadeInRight">
						<div class="col-md-12" >
							<img class="img-md-responsive" alt="Sign Language Highlight" src="sign_language/Highlight.jpg">
							<b>This project is also published as a paper in COMMIT (COMMUNICATION AND INFORMATION TECHNOLOGY) JOURNAL with a title of <a href="https://journal.binus.ac.id/index.php/commit/article/view/8520">Hand Symbol Classification for Human-Computer Interaction Using the Fifth Version of YOLO Object Detection</a>.</b>
							<br><br>
							<h3>Abstract</h3>
							<p>Human-Computer Interaction (HCI) nowadays mostly uses physical contact,  such as people using the mouse to choose something in an application. In the meanwhile, there are certain problems that people will face if using conventional HCI. For example, when people need to hit submit button in an online form with the mouse, but their hands are dirty or wet this could cause the mouse to become dirty or maybe broken. In this research, the authors try to overcome some of the problems while people use conventional HCI by using the Computer Vision method. This research focuses on creating and evaluating the object detection model for classifying hand symbol using YOLO fifth version. The hand gesture classes that were made in this research are 'ok', 'cancel', 'previous', 'next', and 'confirm'. The architecture of YOLO fifth version which is used in this research is yolov5m. After all, the performance of the models for classifying hand symbols is 80% for accuracy, 95% for precision, 84% for recall, and 89% for F1 score.</p>
						</div>	
					</div>
					<!-- YOLOV Algorithm -->
					<div class="row animate-box" style="margin-bottom: 2.5em;" data-animate-effect="fadeInRight">
						<div class="col-md-12">
							<h3>YOLO Algorithm</h3>
							<div class="row">
								<div class="col-md-12">
									<p>
										<a href="https://pjreddie.com/yolo/#:~:text=You%20only%20look%20once%20(YOLO,real%2Dtime%20object%20detection%20system.">YOLO</a> is one of the most popular computer vision algorithms to detect and classify objects in an image or video. The name itself stands for You Only Look Once, and YOLO is categorized as Single-Shot Detector (SSD) Algorithm. In therm of speed inference, the SSD algorithm outperforms other algorithms like R-CNN, Faster R-CNN, etc. However, the drawback is the accuracy is not good as R-CNN. Joseph Redmon developed the YOLO algorithm, which was popular, starting with YOLO Version 3, and uses the darknet framework; the later version of YOLO Version 4 increased the speed and accuracy. Later on, <a href="https://github.com/ultralytics/yolov5">YOLO Version 5</a> was developed using PyTorch by Glenn Jocher, and this version is used in this algorithm.
                                    </p>
                                    <p>
                                        YOLO has a working method of dividing an image into several small pieces: S × S grids. Each grid will be responsible for the centre point of the objects in each grid. The algorithm will give a bounding box and the confidence value of the detected object class in the grid. A bounding box consists of Width, Height, Class, and the bounding box centre (bx, by). When detecting an object, the confidence value is the same as the Intersection Over Union (IOU) obtained from the calculation between the bounding box predictions and ground truth.
										This experiment uses the YOLO Version 5, specifically <b>yolov5m</b>. The architecture of yolov5m consists of backbone, neck, and head which could be visualized as follow:
										<br><br><img class="img-md-responsive" alt="Yolov5m architecture" src="sign_language/YOLOv5_Architecture.jpg">
									</p>
								</div>
							</div>
						</div>
					</div>
					<!-- Model Creation -->
					<div class="row animate-box" style="margin-bottom: 2.5em;" data-animate-effect="fadeInRight">
						<div class="col-md-12">
							<h3>Model Creation</h3>
							<div class="row">
								<div class="col-md-12">
									The model creation divided into three parts: dataset creation, train the data, and evaluate the model.
									The dataset consists of five sign languages those are: Cancel, Previous, Next, Ok, and Confirm. Each class has approximately 100 images, which the model will learn in the training phase.
								</div>
							</div>
							<b>
								<div class="row" style="margin-left: 1%; margin-right: auto;">
									<br>
									<div class="col-xs-4"><img class="img-side-img" alt="Cancel" src="sign_language/cancel.jpg"><p class="text-center">Cancel</p></div>
									<div class="col-xs-4"><img class="img-side-img" alt="Confirm" src="sign_language/confirm.jpg"><p class="text-center">Confirm</p></div>
									<div class="col-xs-4"><img class="img-side-img" alt="Previous" src="sign_language/previous.jpg"><p class="text-center">Previous</p></div>
								</div>
								<div class="row text-center" style="margin-left: 5%; margin-right: auto;">
									<br>
									<div class="col-xs-6"><img class="img-side-img" alt="Ok" src="sign_language/ok.jpg"><p class="text-center">Ok</p></div>
									<div class="col-xs-6"><img class="img-side-img" alt="Next" src="sign_language/next.jpg"><p class="text-center">Next</p></div>
								</div>
							</b>
							<div class="row">
								<div class="col-md-12">
									<p>
										<img class="img-wrap-txt-right" src="sign_language/labelling.png">
										The images itself was captured from the laptop's camera, then processed using <a href="https://github.com/HumanSignal/labelImg">labelimg</a> and saved as YOLO format data consisting of x, y, w, and h of the object. The labelling process was done by creating a boundary box for each image, and the size should be as fit as possible to the object, so the background won't be seen too much.
									</p>
								</div>
							</div>
						</div>
					</div>
					<!-- Train & Evaluate -->
					<div class="row animate-box" style="margin-bottom: 2.5em;" data-animate-effect="fadeInRight">
						<div class="col-md-12">
							<h3>Train & Evaluation Phase</h3>
							<div class="row">
								<div class="col-md-12">
									During the training phase, the images were trained on Google Colab utilizing GPU (Graphics Processing Unit). The Pre-Trained YOLO Version 5 model that was used for training is yolov5m. The training process has parameter as follow: 
									<ul>
										<li>Image size = 416 px</li>
										<li>Batch Size = 4</li>
										<li>Epochs = 300</li>
										<li>Cache = True</li>
										<li>Optimizer = Stochastic Gradient Descent (SGD)</li>
									</ul>
									Also some hyperparameter as follow:
									<ul>
										<li>lr0 = 0.01</li>
										<li>lrf = 0.1</li>
										<li>momentum = 0.937</li>
										<li>weight_decay = 0.0005</li>
										<li>warmup_epochs = 3.0</li>
										<li>warmup_momentum = 0.8</li>
										<li>warmup_bias_lr = 0.1</li>
										<li>box = 0.05</li>
										<li>cls = 0.5</li>
										<li>cls_pw = 1.0</li>
										<li>obj = 1.0</li>
										<li>obj_pw = 1.0</li>
										<li>iou_t = 0.2</li>
										<li>anchor_t = 4.0</li>
										<li>fl_gamma = 0.0</li>
										<li>hsv_h = 0.015</li>
										<li>hsv_s = 0.7</li>
										<li>hsv_v = 0.4</li>
										<li>degrees = 0.0</li>
										<li>translate = 0.1</li>
										<li>scale = 0.5</li>
										<li>shear = 0.0</li>
										<li>perspective = 0.0</li>
										<li>flipud = 0.0</li>
										<li>fliplr = 0.5</li>
										<li>mosaic = 1.0</li>
										<li>mixup = 0.0</li>
										<li>copy_paste = 0.0</li>
									</ul>
									The training was done for about 4 hours and 24.72 minutes. Following the training process, we had Precision, Recall, F1 Score, and mAP graphs, which means we have an evaluation graph of the trained model. The model has 92.7% accuracy, with an IOU threshold between 0.5 and 0.95; a Precision of 99.90% shows the model’s ability to classify hand symbols according to their class; a recall value of 100%, it interprets the model’s ability to predict each time an object appears; an F1 score of 99%.
									<img class="img-md-responsive" alt="Evaluation Graphics" src="sign_language/evaluation.png" style="max-width: 70%;">
								</div>
							</div>
						</div>
					</div>
					<!-- Experiments -->
					<div class="row animate-box" style="margin-bottom: 2.5em;" data-animate-effect="fadeInRight">
						<div class="col-md-12">
							<h3>Experiments</h3>
							<div class="row">
								<div class="col-md-12">
									The experiment was conducted by inference through a live image from a webcam by testing each class from a hand symbol 30 times. Tests were carried out at 1-second intervals for each detection, and the dominant class detected was determined. Then the inference result was evaluated manually by calculating the Accuracy, Precision, Recall, and F1 Score. 
									To calculate the value we need True Positive, True Negative, False Negative, and False positive. The equation for each matrix is as follows:
									<img class="img-md-responsive" alt="Equation" src="sign_language/equation.png">
									The inference resulted in a Confusion Matrix with details as follows:
									<img class="img-md-responsive" alt="Confusion Matrix" src="sign_language/ConfusionMatrix.png">
									Which then from the Confusin Matrix we calculated the Accuracy, Precision, Recall, and F1 Score. The result is as follow:
									<img class="img-md-responsive" alt="Inference Result" src="sign_language/InferenceResult.png" style="max-width: 70%;">
									In conclusion, the model has an accuracy of 80%, a precision of 95%, a recall of 84%, and an F1 score of 89%. As you can see, the result of the inference is lower than the result in the training process, this is because the inference process has some variables that affect the model's ability such as the lighting, the background, and the angle of the camera.

								</div>
							</div>
						</div>
					</div>
					<!-- Acknowledgement -->
					<div class="row animate-box" style="margin-bottom: 2.5em;" data-animate-effect="fadeInRight">
						<div class="col-md-12">
							<h3>Contributing Organizations & People</h3>
							<div class="row">
								<div class="col-md-12">
									<p>This research was successfully conducted with the help of some people. I would like to thank:</p>	
									<ul>
										<li><a href="https://uk.linkedin.com/in/indar-sugiarto">Dr. Indar Sugiarto</a> as my supervisor and the co-author.</li>
										<li><a href="https://petra.ac.id/faculty/12">Fakultas Teknologi Industri</a> as well as <a href="https://lppm.petra.ac.id/">Lembaga Penelitian dan Pengabdian Masyarakat of Petra Christian University</a> for supporting our work through the special research grant No.09/HBK-PENELITIAN/LPPM-UKP/XI/2022.</li>
										<li>The authors also acknowledge the financial support from the <a href="https://www.kemdikbud.go.id/">Ministry of Education, Culture, Research, and Technology of Indonesia</a> under the research grant No. 02/AMD/SP2H/PT-L/LL7/2022.</li>
										<li>and other people who I couldn't mention one by one.</li>
									</ul>
								</div>
							</div>
						</div>
					</div>
					
				</div>
				<!-- End of the project explanation -->
			</section>


		</div><!-- end:colorlib-main -->
	</div><!-- end:container-wrap -->
	</div><!-- end:colorlib-page -->

	<!-- jQuery -->
	<script src="../js/jquery.min.js"></script>
	<!-- jQuery Easing -->
	<script src="../js/jquery.easing.1.3.js"></script>
	<!-- Bootstrap -->
	<script src="../js/bootstrap.min.js"></script>
	<!-- Waypoints -->
	<script src="../js/jquery.waypoints.min.js"></script>
	<!-- Flexslider -->
	<script src="../js/jquery.flexslider-min.js"></script>
	<!-- Owl carousel -->
	<script src="../js/owl.carousel.min.js"></script>
	<!-- Counters -->
	<script src="../js/jquery.countTo.js"></script>
	
	
	<!-- MAIN JS -->
	<script src="../js/main.js"></script>

	</body>
</html>

